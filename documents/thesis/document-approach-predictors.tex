We wish to predict useful dependencies for a proof goal, or conjecture.
For this we define predictors:

\begin{definition}
  A predictor $P$ is a function which given an unproven conjecture $c \in \types$
  computes the likelyhood a dependency $\phi \in \depset$ is useful in proof of conjecture $c$.
  $$
    P : \types \rightarrow \depset \rightharpoonup \mathbb{R}
  $$
\end{definition}

A weak total ordered subset of definitions useful for conjecture $c$ is formed by sorting on the resulting value of type $\mathbb{R}$.
In the next few chapters we define a few of these predictors.

\subsubsection{\knn}

\todo{reference}
\todo{intuition}

For the \knn predictor several prerequisite definitions are required.

\begin{definition}\defgls{dist}\label{def:dist}
  Given two types $x, y \in \types$, the euclidean distance between these types can be computed using the features $\features{}{a}$ and $\features{}{b}$ of those types:
  $$ \text{dist}(x, y) = \left( \sum_{i \in \featurekeys} \left( \features{}{x}(i) - \features{}{y}(i) \right)^2 \right)^{\frac{1}{2}} $$
\end{definition}

For a conjecture $c \in \types$ the euclidean distance using $\text{dist}$ is computed for each known \coqobj $s \in \objs$.
A total order of these \coqobjs is constructed using the following ordering:

\begin{definition}
  $$
    \text{closer}_\phi(x, y) = \text{dist}(x, \phi) \leq \text{dist}(y, \phi)
  $$
\end{definition}

\begin{definition}\glsadd{infimum}\label{def:infimum}
  Given a partially ordered set $(X, \leq)$,
  the \textbf{infimum} or \textbf{greatest lower bound}, denoted by $\infimum$, is the greatest element in $X$ that is less than or equal to all elements of $X$,
  if such an element exists.
\end{definition}

\begin{definition}\glsadd{nth}\label{def:nth}
  Given a totally ordered set $(X, \leq)$ and a number $n \in \nat$, we define
  $$
    \nth{n}{X} = \left\{
      \begin{array}{ll}
        \infimum_X & \text{if}~n = 0 \\
        \nth{n-1}{X \cap \infimum_X} & \text{if}~n > 0 \\
      \end{array}
    \right.
  $$
\end{definition}

\begin{definition}\glsadd{downset}\label{def:downset}
  Given a partially ordered set $(X, \leq)$ with an element $x \in X$, a \textbf{down set} $\downset{X}{x}$ is a subset $L$ with if $y \leq x$, then $y \in L$.
\end{definition}

The $K \in \nat$ closest objects according to this distance measure are selected, which we call $\text{closest}_K(c)$.
\begin{definition}
  $$
    \text{closest}_K(c) = \downset{S}{\nth{K}{(S, \text{closer}_c)}}
  $$
\end{definition}

Their dependencies $\phi \in \depset$ are then suggested by the predictor.
The usefulnes of each suggestion is dependant on the distance of their parents to conjecture $c$.
\begin{definition}
  $$
    P^K_\text{knn}(c, \phi) = \sum_{x \in \text{closest}_K(c)} \deps{x}(\phi) \times \text{dist}(c, x)
  $$
\end{definition}

In future work we might experiment with different distance measures.

\subsubsection{\knnadaptive}

\todo{reference}

An alternative to selecting a predefined $K$ number of neighbors, is keep selecting neighbors until a preset number of suggestions can be made.
This strategy is used by the \knnadaptive predictor.

\begin{definition}
  Given $p \in \nat \rightarrow 2$:

  $$
    \begin{array}{ll}
      \text{first}'(p, i) & = \left\{
      \begin{array}{ll}
         i & ~\text{if}~~ p(i) \\
         \text{first}'(p, i+1) & ~\text{otherwise} \\
      \end{array}
      \right. \\
      \\
      \text{first}(p) & = \text{first}'(p, 0)
    \end{array}
  $$
\end{definition}

\begin{definition}
  Given the $K \in \nat$ closest neighbors of conjecture $c \in \types$ we define the set of suggestions to be

  $$
    S(c, K) = \bigcup_{x \in \text{closest}_K(c)} \left\{ \phi \in \depset ~|~ \deps{x}(\phi) > 0 \right\}
  $$
\end{definition}

\begin{definition}
  $$
    P_\text{aknn}(c, \phi) = P^{\text{first}(\lambda K . |S(c, K)| \geq 1024)}_\text{knn}(c, \phi)
  $$
\end{definition}

\subsubsection{\nb}

Given dependency candidate $\phi \in \depset$, we compute for all features $x \in \featurekeys$:
\[
  W = \tau + \sum_{x \in \featurekeys} w_x ~~\text{with}~~ w_x = \tau + \sum_{\psi \in \parents[\phi]} \left( \features{}{\psi}(x) > 0 \right)
\]

Now we can compute the likelyhood of candidate $\phi$ given some constants $\pi, \sigma, \tau$:
\[
	P_\text{nb}(c, \phi) = \ln W +
  \sum_{x \in \featurekeys | w_x = 0} \features{}{c}(x) \times \sigma +
  \sum_{x \in \featurekeys | w_x \neq 0} \features{}{c}(x) \times \ln(\pi \times w_x) - \ln(W)
\]

By K\"uhlwein \cite{kuhlwein2013mash} it is suggested to use $\pi = 10$, $\sigma = -15$ and $\tau = 20$.

\subsubsection{Adarank}
The premise selection problem is similar to \ltr in information retrieval
as in both cases relevant objects are sought from a large corpus.
Ordering these objects by relevance (or `rank') is also a common theme in both fields of study.

In order to determine in what manner these problems differ, and whether solutions
from information retrieval could be transformed to also work for the premise selection problem,
the Adarank algorithm \cite{xu2007adarank} is used as a showcase.

\ltr is typically used in search engines to search for relevant documents.
Given a search query $\query \in \queries$ composed of several terms, a preselection on the set of
all documents is made. Given a document in this subset $\doc \in \docs{\query}$,
the documents are sorted by rank $\rankdoc{\doc} \in \rat$.
The rank is determined for a document relative to the query.
Typically a document which mainly discusses terms from the query is rated highly.
The information retrieval method determines how this rank is computed.

Premise selection has neither the concepts of queries nor documents.

\begin{definition}
  Queries are formed from the features of each object $s \in \trainset$.
  \[
    \forall_{s \in \trainset} \features{}{s} \in \queries
  \]
\end{definition}

\begin{definition}
  We define the documents (that which we seek to rank) to be analogous to our dependencies.
  \[
    \forall_{\query \in \queries}~ \docs{\query} \subseteq \depset
  \]
\end{definition}

For premise selection, we search for useful dependencies given a conjecture $c \in \types$.
We reason over the types of objects $\objs$, instead of over the types of the dependencies.
Thus in order to transform to \ltr, we need to reason over the types of objects related to each dependency.

\begin{definition}
  For dependency $d \in \depsym$ we define the features $G_d$ to be the sum of
  the features $i \in \featurekeys$ of the parents $\parents[d]$ of $d$:
  \[
    G_d(i) = \sum_{p \in \parents[d]} \features{}{p}(i)
  \]
\end{definition}

\begin{definition}
  Given a query $\query \in \queries$ and document $d \in \docs{\query}$ the feature
  vector is defined as:

  \[
    {\vec{x}}_{q d} = \Psi(\query, d) \in \mathcal{X}
  \]
\end{definition}

\paragraph{Features}

\begin{definition}
  \[ c(w, d) \]
\end{definition}

\begin{definition}
  \[ C \]
\end{definition}

\begin{definition}
  \[ \text{idf}(\cdot) \]
\end{definition}

\begin{definition}
  \[ \text{BM25 score} \]
\end{definition}

\begin{definition}
  \[
  \begin{array}{l}
    \psi_0(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(c(w_i, d) + 1) \\
    \psi_1(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(\frac{|C|}{c(w_i, C)} + 1) \\
    \psi_2(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(\text{idf}(w_i)) \\
    \psi_3(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(\frac{c(w_i, d)}{|d|} + 1) \\
    \psi_4(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(\frac{c(w_i, d)}{|d|} \cdot \text{idf}(w_i) + 1) \\
    \psi_5(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(\frac{c(w_i, d) \cdot |C|}{|d| \cdot c(w_i, C)} + 1) \\
    \psi_6(q, d) = \text{ln}( \text{BM25 score} )
  \end{array}
  \]

  \[
    \Psi(q, d) = \left\{ \psi_0(q, d), ~\ldots~, \psi_6(q, d) \right\}
  \]
\end{definition}

\subsubsection{Ensembles}
\todo{explain}

\subsection{Example}

\begin{lstlisting}[language=Coq, mathescape, frame=none]
$\defs = \{$nat$,$ 0$,$ S$,$ plus$\}$
\end{lstlisting}

\begin{lstlisting}[language=Coq, mathescape, frame=none]
$\thms = \mathtt{dom}(S) \setminus \defs = \{$nat_ind$,$ plus_0_r$\}$
\end{lstlisting}
