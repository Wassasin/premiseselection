\subsection{Relating to previous work}

One of the goals our thesis is to at least reach the same level of performance compared to the work done by Kaliszyk. \cite{kaliszyk2014machine}
For this purpose our experiments are at least set up in a comparable way.
For example, in the experiments by Kaliszyk also 10-fold \crossvalidation is used, along with
something like the canonical consistensize method.
Their final results are also presented for the \corn dataset.

\begin{figure}[H]
  \begin{tabular}{lcccccc}
    - & \oocover & (ours) & & \auc & (ours) & \\
    \hline
    \input{data/relative}
  \end{tabular}
  \caption{The relative performances of both the experiments in this thesis and the experiments as done by Kaliszyk.}
\end{figure}

Our \oocover performance is markedly lower, and can not be explained away by minor differences in approach or parameters.
We discussed this discrepancy with Kaliszyk, but were not able to reach a conclusion as to why this is the case.
Unfortunately their code is not available for further research.
Surprisingly our \auc performance is quite similar.
However as discussed earlier this does not help with regards to the premise selection problem.
A small victory is achieved with \adarank, as it manages to get a better \auc performance.
However this is insignificant as long as this unexplained discrepancy exists.

\subsection{Reliability}
The results compared to the work by Kaliszyk et al \cite{kaliszyk2014machine} bring the reliability of this thesis into question.
There are several key differences compared to our approach.
We have tried to account for these differences by measuring their effect.

\subsubsection{Prior knowledge}

\subsubsection{Consistencize}

\subsection{Machine learning approaches}

\knnadaptive does not work at all.
\nb works OK given correct parameters, but difficult to find.
\adarank works quite well but is expensive computationally and memory-wise.
\ensemble has promise, but haven't dabbled too much.

\subsection{Corpora}
Depth seems to work best. Probably more descriptive of what is going on.
Frequency is OK.
Flat is not descriptive.

Coq is relatively easy.
\todo{The rest depends on prior experiments}

\subsection{Future work}
\subsubsection{Deep learning}
Josef Urban is experimenting with Deep Learning \cite{loos2017deep}, and shows a lot of promise.

\subsubsection{Ensembles}
Ensembles showed promise, but would need to have proper basis.
Currently not terribly important because of bad performance relative to prior work.

\todo{Combinining Depth+Frequency}