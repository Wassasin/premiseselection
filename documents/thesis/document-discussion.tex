\subsection{Relating to previous work}

One of the goals of our thesis is to at least reach the same level of performance compared to the work done by Kaliszyk. \cite{kaliszyk2014machine}
For this purpose our experiments are set up in a comparable way.
For example, in the experiments by Kaliszyk also 10-fold \crossvalidation is used, along with
something comparable to the canonical consistensize method.
Their final results are also presented for the \corn dataset.

\begin{figure}[H]
  \begin{tabular}{lcccccc}
    - & \oocover & (ours) & & \auc & (ours) & \\
    \hline
    \input{data/relative}
  \end{tabular}
  \caption{The relative performances of both the experiments in this thesis and the experiments as done by Kaliszyk.}
\end{figure}

Our \oocover performance is markedly lower, and can not be explained away by minor differences in approach or parameters.
We discussed this discrepancy with Kaliszyk, but were not able to reach a conclusion as to why this is the case.
Unfortunately their code is not available for further research.
Surprisingly our \auc performance is quite similar.
However as discussed earlier this does not help with regards to the premise selection problem.

\subsection{Reliability}
The results compared to the work by Kaliszyk et al \cite{kaliszyk2014machine} bring the reliability of this thesis into question.
There are two key differences compared to our approach.
We have tried to account for these differences by measuring their effect.

\subsubsection{Prior knowledge}
We would expect the machine learning methods to perform better given the prior knowledge of corpora on which is depended.
This however does not seem to be the case.
The research by Kaliszyk \cite{kaliszyk2014machine} only states that 10-fold \crossvalidation is used, but not if the prior
datasets are also tested as part of the testset.
Thus it is difficult to relate our results to that by Kaliszyk.

As discussed in the results in Section \ref{section:prior}, we believe that the prior datasets introduce
too much noise for the prediction methods to handle.
This is purely conjecture, but we believe that this would be less of a problem when
better descriptive features would be used.
Currently each definition forms a feature, but a singular definition does not directly relate
to which theorems would be useful.
It is up to the model to internally relate definitions to useful theorems.
If there is no prior context of usages of a theorem in relation to said definition,
a model would not have a way to build these internal relations.
Prior datasets increase the set of theorems and definitions, but do not necessarily
generate a prior context which relates to the theorems of the testset.
Thus they might make the problem to solve harder.
Better descriptive features would be more abstract, but might provide a better context that follows
naturally from that abstraction.
Concretely the \emph{structure} of a theorem might help as a better feature.

\subsubsection{Poset consistency}
The canonical method performs quite a bit worse than the pessimistic method.
As mentioned in Section \ref{section:poset-consistency}, the canonical method performs
worse because it removes more possibly relevant context from the trainingset compared to the
pessimistic method.
Which part is removed exactly depends on the random seed determining the dataset order used
in the canonical method, whereas for the pessimistic method only (always the same) part of
illegal theorems and definitions is removed.
For the canonical method it is possible to come up with a different seed (and thus a different ordering)
that yields wildly different results.
In this thesis a singular random seed is consistently used for all of the results.

The pessimistic method is not a fair comparison to the prior research, as it finds
the optimal ordering (or even better). What it however does provide is an upper
bound on the performance of the premise selection tools, as well as a novel less biased
way of comparing similar solutions to the premise selection problem.
Whilst the canonical method might yield worse results due to its
random seed, the results from the pessimistic method still do not hold a candle to
the results by Kaliszyk.
Thus we can conclude that regardless of the random seed used, our solution will
not yield a better result than that by Kaliszyk.

\subsection{Machine learning approaches}
Our main contribution to the premise selection problem for \coq is the implementation of various machine learning methods.
We have discovered that the \knnadaptive method performs badly, regardless of corpus used.
The \nb method works OK given suitable parameters.
However these suitable parameters are hard to find.
Our novel adaptation of \adarank works quite well, but is expensive computationally and memory-wise.
Our quick \ensemble experiment shows promise, but we haven't dabbled with various combinations all too much.

\adarank is interesting in that it successfully maps solutions from
the \emph{Learning to rank} domain to our \emph{Premise selection} domain.
However the amount of resources that it requires results in that it can not be feasibly run
with the current implementation or hardware.
As such were we to implement a Clippy for \coq based on our experiments, it would be with a learner based on \ensemble.

\subsection{Frequency, depth, flat models}
The depth model seems to work best, as it is probably a more descriptive model of the dataset.
The frequency model works fine.
The flat model is not descriptive at all, and as such not useful.
It would be interesting to see if models based on both frequency and depth will be significantly better.

\subsection{Corpora}
Given the performance of our models it is hard to say anything about the various corpora used,
except infer their triviality.
For each corpus we've summarised the performance in the following table:

\begin{figure}[H]
  \begin{tabular}{lllrrr}
    Corpus & $|S|$ & $|\defs|$ & $|\thms|$ & \oocover & \auc \\\hline
    \input{data/counts-performance}
  \end{tabular}
  \caption{Corpora (depth model) examined in this thesis, number of objects, definitions and theorems and corresponding best performance
    for any machine learning method with the canonical strategy}
\end{figure}

Surprisingly \formalin seems to be most trivial, followed by \coq, \mathclasses, \corn and finally \mathcomp.
In retrospect \formalin is mostly comprised of mechanical proofs concerning language semantics, and thus would indeed contain the more trivial theorems.
This relation is not linear with regards to their size.

\subsection{Tooling}
We did not integrate our tooling into the CoqIDE GUI, as was one of the design goals for the \roerei tooling.
As we quickly switched to C++ for the prediction component of \roerei, it is doubtful \roerei would ever be merged into the \coq main branch,
even if the performance was significant.
Due to the disappointing performance we decided to not invest the effort to actually implement an assistant like Clippy within CoqIDE.

\subsection{Future work}
\subsubsection{Deep learning}
Sarah Loos et al have experimented with proof guidance and clause selection for \mizar using Deep Learning \cite{loos2017deep}.
Their research show a lot of promise.
Adapting their work to the premise selection for \coq problem however is not straightforward, as the clause selection used is
dependent on the First Order logic nature of \mizar and on operations on proofs in the Clausal Normal Form (CNF).
Alternatively our transformation of the Learning to Rank problem for \adarank could be applied to previous Deep Learning solutions like
those by Song \cite{song2018deep}.

\subsubsection{Ensembles}
For this thesis we've ran a small experiment using ensemble learning, which worked moderately well compared to
the more simple solutions.
With more solutions in the core, quite a few variants of ensembles can be tried out.
Particularly different ensembles can be built from models using both the depth and frequency datasets.
Also ensembles can be made where a part of the ensemble is only aware of local theorems, and others have a wide knowledge base.
I think this might work because in some cases the models that did not learn from prior datasets performed better.

\subsection{Conclusion}
We have developed a method to extract the necessary information from the Coq system in order to perform premise selection
inspired by the work of Kaliszyk and described in Section \ref{section:extraction}.
Several variants of this have been developed, of which the depth-model currently works best.

Of our experiments the Adarank-method has the best performance, but requires significant amounts of resources.
If resources are of import, than our single ensemble method works best.
The Adarank method hints that other Learning-to-Rank solutions can also be transformed and might also perform well.

In retrospect our comparison of various corpora is of limited use by itself.
Combined with an analysis of the methods most notably the \knnadaptive method performs similar in simple cases,
but is unable to keep up for complex corpora such as \corn.
The dependency of prior corpora also gives insight into the incapability of the methods to make use
of a larger context.

\subsection{Reflection}
In academic works any mistake can impact the results significantly.
This is especially true for work that applies machine learning, as machine learning is a statistical black box which
is difficult to reason about.
During the implementation of this thesis I often wondered if the results that were published in the various papers I read
were actually correct, or whether the results were flawed due to bugs in for example the implementation of the
performance metrics.
In various cases I am not able to reproduce the results published as the technical descriptions lack the necessary detail
to re-implement the described experiments.
Even when speculating on these implementation details, sub-par performance results are achieved.
As a result I have become sceptical about machine learning as an academic field.

I think it is absolutely critical for the machine learning field, indeed all of academia,
that reproducibility but also falsification of prior results gains more importance.
Currently it is almost impossible to get a paper published that re-affirms prior results.
In Computing Science the horrors of \emph{PhD-code}, code which is of abhorrent bad quality, are also well known.
There is also academic code that is fine, but surely it is a rarity.
A lot could be learned from the principles of Open Source software, where code is typically written
in an universally understandable manner and published in a standardised way.
I would even go further as to state that all published academic works should also publish
their complete datasets and instrumentation (in the case of Computing Science: the code)
for the sake of efficiency and integrity.

I have done my utmost best, given limited time, to enable the full reproducibility of
the datasets and ultimately the results of this thesis.
Even though it is highly likely that there is a critical error in either the method
or the code and that thus my results are of no use,
I sincerely hope that my description and code will still be of some use to someone.