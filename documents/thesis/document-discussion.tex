\subsection{Relating to previous work}

They also use 10-fold \crossvalidation.
Something like the canonical consistensize method.
On the \corn dataset.

\begin{figure}[H]
  \begin{tabular}{lcccc}
    - & \oocover & (ours) & \auc & (ours) \\
    \hline
    \knnadaptive & 0.723 & 0.367 ({\color{red}-49.2\%}) & 0.834 & 0.668 ({\color{red}-19.9\%}) \\
    \nb & 0.726 & 0.520 ({\color{red}-28.3\%}) & 0.836 & 0.732 ({\color{red}-12.4\%}) \\
    \ensemble & 0.749 & 0.528 ({\color{red}-29.5\%}) & 0.849 & 0.762 ({\color{red}-10.2\%}) \\
  \end{tabular}
\end{figure}

The performance is markedly lower, and can not be explained away by minor differences in approach or parameters.
We discussed this discrepancy with Kaliszyk, but were not able to reach a conclusion as to why this is the case.
Unfortunately their code is not available for further research.

\subsection{Reliability}
The results compared to the work by Kaliszyk et al \cite{kaliszyk2014machine} bring the reliability of this thesis into question.
There are several key differences compared to our approach.
We have tried to account for these differences by measuring their effect.

\subsubsection{Prior knowledge}

\subsubsection{Consistencize}

\subsection{Machine learning approaches}

\knnadaptive does not work at all.
\nb works OK given correct parameters, but difficult to find.
\adarank works quite well but is expensive computationally and memory-wise.
\ensemble has promise, but haven't dabbled too much.

\subsection{Corpora}
Depth seems to work best. Probably more descriptive of what is going on.
Frequency is OK.
Flat is not descriptive.

Coq is relatively easy.
\todo{The rest depends on prior experiments}

\subsection{Future work}
\subsubsection{Deep learning}
Josef Urban is experimenting with Deep Learning \cite{loos2017deep}, and shows a lot of promise.

\subsubsection{Ensembles}
Ensembles showed promise, but would need to have proper basis.
Currently not terribly important because of bad performance relative to prior work.

\todo{Combinining Depth+Frequency}