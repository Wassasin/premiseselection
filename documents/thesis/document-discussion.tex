\subsection{Relating to previous work}

One of the goals of our thesis is to at least reach the same level of performance compared to the work done by Kaliszyk. \cite{kaliszyk2014machine}
For this purpose our experiments are set up in a comparable way.
For example, in the experiments by Kaliszyk also 10-fold \crossvalidation is used, along with
something comparable to the canonical consistensize method.
Their final results are also presented for the \corn dataset.

\begin{figure}[H]
  \begin{tabular}{lcccccc}
    - & \oocover & (ours) & & \auc & (ours) & \\
    \hline
    \input{data/relative}
  \end{tabular}
  \caption{The relative performances of both the experiments in this thesis and the experiments as done by Kaliszyk.}
\end{figure}

Our \oocover performance is markedly lower, and can not be explained away by minor differences in approach or parameters.
We discussed this discrepancy with Kaliszyk, but were not able to reach a conclusion as to why this is the case.
Unfortunately their code is not available for further research.
Surprisingly our \auc performance is quite similar.
However as discussed earlier this does not help with regards to the premise selection problem.

\subsection{Reliability}
The results compared to the work by Kaliszyk et al \cite{kaliszyk2014machine} bring the reliability of this thesis into question.
There are several key differences compared to our approach.
We have tried to account for these differences by measuring their effect.

\subsubsection{Prior knowledge}
We would expect the machine learning methods to perform better given the prior knowledge of corpora on which is depended.
This however does not seem to be the case.
The research by Kaliszyk \cite{kaliszyk2014machine} only states that 10-fold \crossvalidation is used, but not if the prior
datasets are also tested as part of the testset.
\todo{discuss why non-prior has better performance}

\subsubsection{Consistencize}
The canonical method performs quite a bit worse than the pessimistic method.
This is however expected.
\todo{discuss}

\subsection{Machine learning approaches}
\knnadaptive does not work at all.
\nb works OK given suitable parameters. These values are hard to find however.
\adarank works quite well but is expensive computationally and memory-wise.
Also \adarank makes a gratuitious amount of suggestions.
\ensemble shows promise, but we haven't dabbled with various combinations all too much.

\adarank is interesting in that it succesfully maps solutions from
the \emph{Learning to rank} domain to our \emph{Premise selection} domain.
However the amount of resources that it requires results in that it can not be feasibly run
with the current implementation or hardware.
As such were we to implement a Clippy for \coq based on our experiments, it would be with a learner based on \ensemble.

\subsection{Frequency, depth, flat models}
The depth model seems to work best, as it is probably a more descriptive model of the dataset.
The frequency model works fine.
The flat model is not descriptive at all, and as such not useful.
It would be interesting to see if models based on both frequency and depth will be significantly better.

\subsection{Corpora}
Given the performance of our models it is hard to say anything about the various corpora used,
except infer their triviality.
For each corpus we've summarized the performance in the following table:

\begin{figure}[H]
  \begin{tabular}{lllrrr}
    Corpus & $|S|$ & $|\defs|$ & $|\thms|$ & \oocover & \auc \\\hline
    \input{data/counts-performance}
  \end{tabular}
  \caption{Corpora (depth model) examined in this thesis, number of objects, definitions and theorems and corresponding best performance
    for any machine learning method with the canonical strategy}
\end{figure}

Surprisingly \formalin seems to be most trivial, followed by \coq, \mathclasses, \corn and finally \mathcomp.
In retrospect \formalin is mostly comprised of mechanical proofs concerning language semantics, and thus would indeed contain the most trivial theorems.
This relation is not linear with regards to their size.

\subsection{Tooling}
We did not integrate our tooling into the CoqIDE GUI, as was one of the design goals for the \roerei tooling.
As we quickly switched to C++ for the prediction component of \roerei, it is doubtful \roerei would ever be merged into the \coq main branch,
even if the performance was significant.
Due to the disappointing performance we decided to not invest the effort to actually implement an assistant like Clippy within CoqIDE.

\subsection{Future work}
\subsubsection{Deep learning}
Josef Urban is experimenting with Deep Learning \cite{loos2017deep}, and shows a lot of promise.

\subsubsection{Ensembles}
Ensembles showed promise, but would need to have proper basis.
Currently not terribly important because of bad performance relative to prior work.

\todo{Combinining Depth+Frequency}

\subsection{Conclusion}
We have developed a method to extract the necessary information from the Coq system in order to perform premise selection
inspired by the work of Kaliszyk and described in section \ref{section:extraction}.
Several variants of this have been developed, of which the depth-model currently works best.

Of our experiments the Adarank-method has the best performance, but requires significant amounts of resources.
If resources are of import, than our single ensemble method works best.
The Adarank method hints that other Learning-to-Rank solutions can also be transformed and might also perform well.

\todo{corpus performance relating to methods}
