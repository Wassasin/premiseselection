\subsection{Relating to previous work}

One of the goals of our thesis is to at least reach the same level of performance compared to the work done by Kaliszyk. \cite{kaliszyk2014machine}
For this purpose our experiments are set up in a comparable way.
For example, in the experiments by Kaliszyk also 10-fold \crossvalidation is used, along with
something comparable to the canonical consistensize method.
Their final results are also presented for the \corn dataset.

\begin{figure}[H]
  \begin{tabular}{lcccccc}
    - & \oocover & (ours) & & \auc & (ours) & \\
    \hline
    \input{data/relative}
  \end{tabular}
  \caption{The relative performances of both the experiments in this thesis and the experiments as done by Kaliszyk.}
\end{figure}

Our \oocover performance is markedly lower, and can not be explained away by minor differences in approach or parameters.
We discussed this discrepancy with Kaliszyk, but were not able to reach a conclusion as to why this is the case.
Unfortunately their code is not available for further research.
Surprisingly our \auc performance is quite similar.
However as discussed earlier this does not help with regards to the premise selection problem.

\subsection{Reliability}
The results compared to the work by Kaliszyk et al \cite{kaliszyk2014machine} bring the reliability of this thesis into question.
There are several key differences compared to our approach.
We have tried to account for these differences by measuring their effect.

\subsubsection{Prior knowledge}
We would expect the machine learning methods to perform better given the prior knowledge of corpora on which is depended.
This however does not seem to be the case.
The research by Kaliszyk \cite{kaliszyk2014machine} only states that 10-fold \crossvalidation is used, but not if the prior
datasets are also tested as part of the testset.
\todo{discuss why non-prior has better performance}

\subsubsection{Consistencize}
The canonical method performs quite a bit worse than the pessimistic method.
This is however expected.
\todo{discuss}

\subsection{Machine learning approaches}
\knnadaptive does not work at all.
\nb works OK given suitable parameters. These values are hard to find however.
\adarank works quite well but is expensive computationally and memory-wise.
Also \adarank makes a gratuitious amount of suggestions.
\ensemble shows promise, but we haven't dabbled with various combinations all too much.

\adarank is interesting in that it succesfully maps solutions from
the \emph{Learning to rank} domain to our \emph{Premise selection} domain.
However the amount of resources that it requires results in that it can not be feasibly run
with the current implementation or hardware.
As such were we to implement a Clippy for \coq based on our experiments, it would be with a learner based on \ensemble.

\subsection{Frequency, depth, flat models}
The depth model seems to work best, as it is probably a more descriptive model of the dataset.
The frequency model works fine.
The flat model is not descriptive at all, and as such not useful.
It would be interesting to see if models based on both frequency and depth will be significantly better.

\subsection{Corpora}
Given the performance of our models it is hard to say anything about the various corpora used,
except infer their triviality.
For each corpus we've summarized the performance in the following table:

\begin{figure}[H]
  \begin{tabular}{lllrrr}
    Corpus & $|S|$ & $|\defs|$ & $|\thms|$ & \oocover & \auc \\\hline
    \input{data/counts-performance}
  \end{tabular}
  \caption{Corpora (depth model) examined in this thesis, number of objects, definitions and theorems and corresponding best performance
    for any machine learning method with the canonical strategy}
\end{figure}

Surprisingly \formalin seems to be most trivial, followed by \coq, \mathclasses, \corn and finally \mathcomp.
In retrospect \formalin is mostly comprised of mechanical proofs concerning language semantics, and thus would indeed contain the more trivial theorems.
This relation is not linear with regards to their size.

\subsection{Tooling}
We did not integrate our tooling into the CoqIDE GUI, as was one of the design goals for the \roerei tooling.
As we quickly switched to C++ for the prediction component of \roerei, it is doubtful \roerei would ever be merged into the \coq main branch,
even if the performance was significant.
Due to the disappointing performance we decided to not invest the effort to actually implement an assistant like Clippy within CoqIDE.

\subsection{Future work}
\subsubsection{Deep learning}
Sarah Loos et al have experimented with proof guidance and clause selection for \mizar using Deep Learning \cite{loos2017deep}.
Their research show a lot of promise.
Adapting their work to the premise selection for \coq problem however is not straightforward, as the clause selection used is
dependent on the First Order logic nature of \mizar and on operations on proofs in the Clausal Normal Form (CNF).
Alternatively our transformation of the Learning to Rank problem for \adarank could be applied to previous Deep Learning solutions like
those by Song \cite{song2018deep}.

\subsubsection{Ensembles}
For this thesis we've ran a small experiment using ensemble learning, which worked moderately well compared to
the more simple solutions.
With more solutions in the core, quite a few variants of ensembles can be tried out.
Particularly different ensembles can be built from models using both the depth and frequency datasets.
Also ensembles can be made where a part of the ensemble is only aware of local theorems, and others have a wide knowledge base.
I think this might work because in some cases the models that did not learn from prior datasets performed better.

\subsection{Conclusion}
We have developed a method to extract the necessary information from the Coq system in order to perform premise selection
inspired by the work of Kaliszyk and described in Section \ref{section:extraction}.
Several variants of this have been developed, of which the depth-model currently works best.

Of our experiments the Adarank-method has the best performance, but requires significant amounts of resources.
If resources are of import, than our single ensemble method works best.
The Adarank method hints that other Learning-to-Rank solutions can also be transformed and might also perform well.

In retrospect our comparison of various corpora is of limited use by itself.
Combined with an analysis of the methods most notably the \knnadaptive method performs similar in simple cases,
but is unable to keep up for complex corpora such as \corn.
The dependency of prior corpora also gives insight into the incapability of the methods to make use
of a larger context.

\subsection{Reflection}
In academic works any mistake can impact the results significantly.
This is especially true for work that applies machine learning, as machine learning is a statistical black box which
is difficult to reason about.
During the implementation of this thesis I often wondered if the results that were published in the various papers I read
were actually correct, or whether the results were flawed due to bugs in for example the implementation of the
performance metrics.
In various cases I am not able to reproduce the results published as the technical descriptions lack the necessary detail
to re-implement the described experiments.
Even when speculating on these implementation details, subpar performance results are achieved.
As a result I have become sceptical about machine learning as an academic field.

I think it is absolutely critical for the machine learning field, indeed all of academia,
that reproducibility but also falsification of prior results gains more importance.
Currently it is almost impossible to get a paper published that re-affirms prior results.
In Computing Science the horrors of \emph{PhD-code}, code which is of abhorrent bad quality, are also well known.
There is also academic code that is fine, but surely it is a rarity.
A lot could be learned from the principles of Open Source software, where code is typically written
in an universally understandable manner and published in a standardised way.
I would even go further as to state that all published academic works should also publish
their complete datasets and instrumentation (in the case of Computing Science: the code)
for the sake of efficiency and integrity.

I have done my utmost best, given limited time, to enable the full reproducibility of
the datasets and ultimately the results of this thesis.
Even though it is highly likely that there is a critical error in either the method
or the code and that thus my results are of no use,
I sincerely hope that my description and code will still be of some use to someone.