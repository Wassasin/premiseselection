We want to extract \pcic sentences from \coq vernac (.v) files, in order to perform \premiseselection.
For this thesis we want to try various approaches to \premiseselection,
and we want to know how well our solution performs.
That is why we implemented the \premiseselection tool called \roerei, or \emph{scrambled egg} in Dutch.
An analogy for that it scrambles the output of the \coq theorem prover.
The source code can be found on Github\footnote{\url{https://github.com/Wassasin/roerei}} and is Open Source.
Various design goals of this tool are to:
\begin{itemize}
    \item Support offline learning and analysis of \machinelearning on the various corpora.
    \item Enable integration in the \coqide GUI.
		We would like to create something like Clippy (see Figure \ref{figure:clippy}) or Vigor (see Figure \ref{figure:vigor}) for \coq.
		Both are integrated intelligent user interfaces that assist users using an interactive animated character.
    \item Enable merging of the premise selection tool in the \coq main branch as a plugin.
\end{itemize}
Ideally the tooling created can be used as a building block in a future \coq tactic which finishes proofs fully automatically.

\begin{center}
	\begin{minipage}{0.49\linewidth}
		\begin{figure}[H]
			\centering
			\includegraphics[height=10em]{assets/clippy.png}
			\caption{Clippy for Microsoft Word}
			\label{figure:clippy}
		\end{figure}
	\end{minipage}
	\begin{minipage}{0.49\linewidth}
		\begin{figure}[H]
			\centering
			\includegraphics[height=10em]{assets/vigor.png}
			\caption{Vigor for VI}
			\label{figure:vigor}
		\end{figure}
	\end{minipage}
\end{center}

As we want \pcic sentences and not \gallina text, directly reading \coq vernac (.v) files will not be very useful.
During compilation of these vernac files \coq however generates \pcic terms internally.
These \pcic terms can be exported as so-called \acic terms using the \xml plugin distributed with \coq:
\begin{definition}[\acic]\defgls{acic}
	The Calculus of (Co)Inductive Constructions with Explicit Named Substitutions \cite{coen2000progettazione}.

	In normal \cic sections can contain local variables.
	After closing such a section, all definitions are discharged by abstracting these local section variables away.
	In \acic this is no longer necessary as the definitions are adapted to explicitly capture local variables.
\end{definition}

The tool is comprised of a \preloader and the main program \roerei.
The \preloader is implemented in \ocaml and handles importing the \xml generated by \coq and emitting summaries.
All other work, such as making predictions and measuring it's own performance is done by \roerei, which is implemented in \cpp.

The \machinelearning field favours \python and \matlab as the preferred development environments.
Thus we needed to re-implement various \machinelearning algorithms in \cpp.

\subsection{Transformations}
\label{section:transformations}
The premise selection tool works as follows, and can be summarised as in Figure \ref{figure:transformations}:
\subsubsection{Predicting}
\begin{enumerate}
	\item Export all \coq objects from the standard library and the selected corpora to XML files during their compilation.
		\coq has an XML export functionality built in, until version 8.4pl5.
		Currently this is the only existing method to extract \coq objects.
		This process is more thoroughly described in Section \ref{section:extraction}.
	\item Read these objects from XML files back into canonical \acic\glsadd{acic} format in memory of the \preloader.
	\item From each \coqobj[s] of the form $\name[s] \objdef \term[s] : \type[s]$,
		yield a summary $<\name[s], \termset{s}, \typeset{s}>$.
		\begin{definition}[$\flatten{x}$]\defgls{flatten}
			For a given term (or type) $x$, yield the set of names referenced to in the term.
			\[ \flattensym : \terms \rightarrow 2^{\names} \]
		\end{definition}
		Given that definition, $\termset{s}$\glsadd{termset} is the set of names of all objects in the term $\term[s]$,
		and $\typeset{s}$\glsadd{typeset} is the set of all names in the type $\type[s]$.
		Note that other variants for this $\flatten{x}$ can be used.
    We can also extract the number of occurances of each name using $\countoccur{x}$\glsadd{countoccur}:
    \begin{definition}[$\countoccur{x}$]\defgls{countoccur}
			For a given term (or type) $x$, yield the number of times names are referenced in the term.
      \[ \countoccursym : \terms \rightarrow \mathbb{N}^{\names} \]
    \end{definition}
    Similarly we can also extract the minimum depth for which each name occurs as $\depthoccur{x}$\glsadd{depthoccur}.
		\begin{figure}[H]
			\[
				\begin{array}{rcl}
					\typeset{\texttt{O}}, \typeset{\texttt{S}}, \typeset{\texttt{nat\_id}}, \typeset{\texttt{plus}} & = & \{ \texttt{nat} \}\\
					\typeset{\texttt{nat\_ind}} & = & \{ 0, \texttt{S} \} \\
					\typeset{\texttt{plus\_0\_r}} & = & \{ \texttt{nat}, \texttt{eq}, \texttt{plus}, 0 \} \\
					& & \\
					\termset{\texttt{nat\_id}} & = & \emptyset \\
					\termset{\texttt{plus\_0\_r}} & = & \{ \texttt{eq\_sym}, \texttt{nat\_ind}, \texttt{eq\_refl}, \texttt{f\_equal}, \texttt{S} \}
				\end{array}
			\]
			\caption{Summaries yielded for the natural number example in Figure \ref{figure:natexample}}
		\end{figure}
		These summaries are written to disk, and read again into memory of \roerei.
	\item Distinguish theorems from other definitions.
		Ideally we would like to take all objects of sort \sortprop to be the theorems.
		However this does not work in all cases, specifically for the \corn dataset.
		In order to solve this we use a heuristic as defined by Kaliszyk et al.\ \cite{kaliszyk2014machine}.
		Where we consider $\objs$ to be the set of all \coq objects, we take
    \[
      \begin{array}{rcll}
        \defs & = & \bigcup_{s \in S} \typeset{s} & \text{(Definition \ref{def:defs})}\glsadd{defs} \\
        \thms & = & (\bigcup_{s \in S} \termset{s}) \setminus \defs & \text{(Definition \ref{def:thms})}\glsadd{thms}
      \end{array}
    \]
		This is more extensively explained in Section \ref{section:thmsdefs}.
		\begin{figure}[H]
			\[
				\begin{array}{rcl}
					\defs & = & \{ 0, \texttt{S}, \texttt{nat}, \texttt{eq}, \texttt{plus} \} \\
					\thms & = & \{ \texttt{eq\_sym}, \texttt{nat\_ind}, \texttt{eq\_refl}, \texttt{f\_equal} \}
				\end{array}
			\]
			\caption{$\defs$ and $\thms$ for the natural number example in Figure \ref{figure:natexample}}
		\end{figure}
    \item From the $\thms$ and $\defs$ generate a \dagraph of dependencies (Section \ref{section:feats})
			and feature vectors (Section \ref{section:deps}) for all \coq objects.
	\item These features and dependencies can now be used to predict by teaching them to a predictor, for example \knn.
		For evaluating the performance for a given predictor method or corpus, we first use \crossvalidation in this step.
		When used \emph{in the field} the entire body of knowledge can be used to teach a predictor.
		See Section \ref{section:predictors} for more on the various predictors.
\end{enumerate}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[auto, node distance=2.5cm, main/.style={draw,align=center}]
		\node[main] (coq) {\coqobj[s]\\inside coq};
		\node[main] (xml) [below of=coq] {XML\\representation};
		\node[main] (term) [below of=xml] {\coqobj[s]\\$\name \objdef \term : \type$};
		\node[main] (summary) [below of=term] {Summary\\$<\name[s], \termset{s}, \typeset{s}>$};
		\node[main] (definitions) [below right=2cm and 0cm of summary] {Definitions\\$\defs$};
		\node[main] (theorems) [below left=2cm and 0cm of summary] {Theorems\\$\thms$};
    \node[main] (features) [below of=definitions] {Features\\of theorems\\$\features{}{s}$};
    \node[main] (dependencies) [below of=theorems] {Dependencies\\of theorems\\$\deps{s}$};
		\node[main] (predictor) [below=7.0cm of summary] {Predictor};

		\draw[->] (coq) edge node {(1) Coq XML export} (xml);
		\draw[->] (xml) edge node {(2) Parser} (term);
		\draw[->] (term) edge node {(3) Resolver} (summary);
		\draw[->] (summary) edge node [right] {(4) $\bigcup_{s \in S} \typeset{s}$} (definitions);
		\draw[->] (summary) edge node [left] {$\bigcup_{s \in S} \termset{s} \setminus \defs$ (4)} (theorems);
		\draw[dashed] (definitions) edge node {(4)} (theorems);
		\draw[->] (theorems) edge node {(5)} (dependencies);
		\draw[->] (definitions) edge node {(5)} (features);
		\draw[->] (features) edge node [left] {(6)} (predictor);
		\draw[->] (dependencies) edge node [right] {(6)} (predictor);
	\end{tikzpicture}
	\caption{Transformations performed by the \premiseselection tooling.}
	\label{figure:transformations}
\end{figure}

\subsubsection{Measuring}
When evaluating the performance of a specific premise selector we also need to a bit more work still.
Given an imported corpus (see Section \ref{section:corpora}) as a graph of dependencies and a set of features,
we need to create varying training sets and a test sets to evaluate our performance on.
We use \crossvalidation to create these datasets (see Section \ref{section:estimating-performance}).
Again various predictors (see Section \ref{section:predictors}) are run, yielding predictions.
The performance of these predictions are evaluated using various metrics (see Section \ref{section:metrics}).

\subsection{Extraction}
\label{section:extraction}
\input{document-approach-extraction}

\subsection{Theorems and definitions}
\label{section:thmsdefs}

For the sake of \premiseselection, we would like to categorize all \coqobjs as either theorems or definitions.
\begin{definition}[theorem]\glsadd{theorem}
    Theorems are \coqobjs that have been proved based on previously established \coqobjs.
    They logically follow from the defined axioms.
\end{definition}

\begin{definition}[definition]\glsadd{definition}
    Definitions are all \coqobjs that are not theorems.
    They are either simply constants or purely transformative operations on such constants
    and can merely assist in establishing new proofs.
\end{definition}

\coq is based on the Curry-Howard isomorphism.
This means that the statements of proofs (what is proven) are encoded in the type of the theorems.
Axioms are then defined as simple constants, and used in the term of a theorem to justify proofs.
Thus on a logical level there is no distinction between a definition and a theorem.
\coq tries to track this distinction by having the type of theorems be of sort \sortprop.
Ideally we would like to take all objects of sort \sortprop to be the theorems.
However this does not work in all cases, specifically for the \corn dataset.
In the case of \corn the propositions are of type \cprop, which is of sort \sorttype.
\todo{example-corn}

In order to solve this we use a heuristic as defined by Kaliszyk et al.\ \cite{kaliszyk2014machine}.
Where we consider $\objs$ to be the set of all \coqobjs,

\begin{definition}\defgls{defs}\label{def:defs}
  we take the definitions to be the names in the types of all \coqobjs
  \[ \defs = \bigcup_{s \in \objs} \typeset{s} \]
\end{definition}

\begin{definition}\defgls{thms}\label{def:thms}
  we take the theorems to be the names in the terms of all \coqobj, excluding all definitions
  \[ \thms = (\bigcup_{s \in \objs} \termset{s}) \setminus \defs \]
\end{definition}

This provides an adequate distinction for our purposes.

\subsection{Features}
\label{section:feats}

For \premiseselection we would like to suggest new theorems (premises) for open proof goals.
These premises are selected according to the type of the proof goal (the names of the constants in the proof goal).
Specifically we call the set of aspects which describe a proof goal the set of feature keys.

\begin{definition}\defgls{featurekeys}\label{def:featurekeys}
  The set of feature keys $\featurekeys$ is defined as the set of definitions, but with possible extensions such as the use of products in a proof goal
  \[ \featurekeys = \defs \cup \{ \times \} \]
\end{definition}

Previously we have defined the $\flattensym$ operation for terms $\terms$ and types $\types$ resulting in $\names$.
We extend this operation to also yield the amount of products.

\[ \flattensym : \types \rightarrow 2^{\featurekeys} \]

Given a proof goal, for each feature key a numeric value can be derived.
Such a value is called a feature.

\begin{definition}\defgls{features}\label{def:features}
  For a \coqobj $s \in \objs$ the features are computed using the function
  $$
  \begin{array}{lcl}
    \features{}{s} & : & 2^\featurekeys \\
    \features{}{s}(x) & = & \flatten{\type[s]}(x)
  \end{array}
  $$
\end{definition}

The functions $\countoccursym$ and $\depthoccursym$ as described in Section \ref{section:transformations} can also be used to compute features for a \coqobj.
We denote these variants of features as $\features{\countsym}{s}$ and $\features{\depthsym}{s}$ with $\features{\countsym}{s} : \nat^\featurekeys$.
In the following text we will not explicitly state which of these variants are used,
as that is only relevant in parts of the results section (Section \ref{section:results}) and onwards.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/nat-features}
	\caption{Features for the natural numbers example depicted as a graph}
\end{figure}

\subsection{Dependencies}
\label{section:deps}

\begin{definition}\defgls{depset}\label{def:depset}
  The set of all possible dependencies $\depset$ is defined as the set of theorems.
  $$
  \depset = \thms
  $$
\end{definition}

The dependencies $\deps{s}$ of a \coqobj[s] are names of other objects which have been used to define or prove $s$.

\begin{definition}\defgls{deps}\label{def:deps}
  For a \coqobj $s \in \objs$ the dependencies are computed using the function
  $$
  \begin{array}{lcl}
    \deps{s} & : & 2^{\depset} \\
    \deps{s}(x) & = & \flatten{\term[s]}(x)
  \end{array}
  $$
\end{definition}

Later we will want to reproduce this set of dependencies given only the original proof goal (or type) of a \coqobj.
We could also compute the depth and number of occurances of these dependencies.
However this is not used in this thesis.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/nat-dependencies}
	\caption{Dependencies for the natural numbers example depicted as a graph}
\end{figure}

\subsection{Predictors}
\label{section:predictors}
\input{document-approach-predictors}

\subsection{Estimating performance}
\label{section:estimating-performance}
The performance of a predictor is estimated by running the prediction on known corpora.
The results of these predictions are quantified using a set of metrics.

Continually running the predictors on the same dataset whilst developing said predictors invites so-called overfitting.
A predictor that overfits has poor predictive performance because the underlying statistical model of the predictor describes noise instead of any relevant relationship.
In order to prevent this overfitting bias, \crossvalidation is used.
\crossvalidation estimates the performance of the predictor for an independant dataset (i.e. from a real problem).

There are quite a few variants of \crossvalidation.
In this thesis we use $k$-fold \crossvalidation with $k = 10$ \cite{kohavi1995study}.
In general $k$-fold \crossvalidation involves partitioning the dataset into several subsets.
These subsets are combined such that there are two complementary subsets: a \emph{trainingset} $\trainset$ and a \emph{testset} $\testset$.
The trainset is used to train a predictor, and the testset is used to perform the prediction on, yielding the performance metrics.
The performance of the predictor is determined by averaging the performance metrics from all complementary combinations.

% Cross validation on Corpora
% `Proof in Progress` -> step (aconstr)
% Subset corpora (Train) -> Corpus (Test) -> Rating
% A corpus is divided into Proofs in Progress (each substep)
% Ratio of guessed steps

For normal classification these subsets are randomly selected.
For \premiseselection this is not adequate as the dataset is not just a set of datapoints, but a tree of definitions which are dependant on eachother.
Randomly selecting from this tree to create a subset would result in an inconsistent dataset.
Given randomly selected subsets, we use three different strategies to make these subsets consistent with regards to the dependants of conjectures.

\subsubsection{Pessimistic}
We consider the dependants matrix $\parents$ to be the dependency matrix $\depsym$ transposed.
\begin{definition}
  \[
    \parents = \depsym^{T}
  \]
\end{definition}

Given a conjecture $\phi \in \testset$ we filter the dependants $\parents$ of $\phi$ transitively out of the trainset $\trainset$ for that prediction:
\begin{definition}
  \[
    \trainset^{\phi} = \trainset \backslash \parentstrans[\phi]
  \]
\end{definition}
The pessimistic method removes the least amount of elements out of the dataset to make it consistent.
It has the least risk of removing relevant elements, but also includes the most irrelevant datapoints.
During the remainder of this thesis we use this method to measure the performance of a predictor, unless otherwise specified.

\subsubsection{Canonical}
In other research first a consistent order is chosen for the definitions.
Given a corpus with a set of source files and a single given entry file, this order is already defined.
The order in which the files are included dictate the order in which terms are defined.

The XML export functionality of \coq only yields a set of terms, not an order in which these have been defined.
Our `canonical' method reconstructs a (random) consistent order, and simulates the in-order learning of terms and testing of conjectures.

First the order for the poset is defined:
\begin{definition}
  Construct a poset of objects from $\objs$ using the dependency relation for $x, y \in \objs$:
  \[
    x <_D y ~~\leftrightarrow~~ \depstrans[x] \cap (\parentstrans[y] \cup \{y\}) \neq \emptyset
  \]
\end{definition}

This poset is then linearized to an ordered list.
The linearization is done in a random manner using Depth-first topological sorting \cite{tarjan1976edge}.
This method comes closest to simulating the behaviour as described by Kaliszyk \cite{kaliszyk2014machine}.

\subsubsection{Optimistic}

In order to measure the effect of leaving out the rest of the corpus
the `optimistic' method constructs a training set where only the bare minimum of entries is included.
This method only includes all absolutely required parents of conjecture $\phi$.

\begin{definition}
  \[
    \trainset^{\phi} = \trainset \cap \parentstrans[\phi]
  \]
\end{definition}

Intuitively our machine learning methods infer the required dependencies by looking at similar previously defined terms.
The `optimistic' method minimizes noise, but also minimizes similar seemingly irrelevant terms which probably end up to be useful.
Thus results from this method will reveal the role of irrelevant terms, but also will show the relevance of similar but not directly related terms.

\subsection{Corpora}
\label{section:corpora}
\input{document-approach-corpora}

\subsection{Metrics}
\label{section:metrics}
\input{document-approach-metrics}
