The premise selection tool works as follows, and can be summarised as in table \ref{table:transformations}:
\begin{itemize}
    \item Export all \coq objects from the standard library and the selected corpora to XML.
		\coq has an XML export functionality built in, until version 8.4pl5.
	\item Parse these objects back into canonical \acic format.
	\item From each Coq term $s$ of the form $s := t : A$, yield a summary $<p_s, t_s, A_s>$:
		where $p_s$ is the canonical name (uri) of the term,
		$t_s$ the names of all objects in the proof body (the proof itself),
		and $A_s$ all names in the type of the proof (what is proven).
		These canonical names are not yet human readable as they are composed of internal identifiers.
		Therefor we separately yield a mapping $p_s \rightarrow p'_s$, where $p'_s$ is a human readible canonical name for the term.
	\item These summaries are written to disk, and read again in the next half of the pipeline due to problems as described in section \ref{section:report}.
	\item Distinguish theorems from other definitions, which yields the features and dependencies.
		For this I use a heuristic as defined by Kaliszyk \cite{kaliszyk2014machine}:
		the set of allowed features follows directly from splitting the set of all objects into either theorems $T$ or definitions $D$.
		Were we consider $S$ to be the set of all coq terms, we consider
		$$D = \bigcup_{s \in S} t_s ~~\text{and}~~ T = \bigcup_{s \in S} A_s \setminus D \text{.}$$
		This is more extensively explained in section \ref{section:features}.
    \item Generate a \dagraph of dependencies for all \coq objects.
		This is especially useful when determining the performance of our solution as our predictions may not use
		knowledge from future (yet unproven) proofs.
	\item These features and dependencies can now be used to predict, using for example \knn.
	\item Predictions are subsequently augmented by further traversing the \dagraph of dependencies.
\end{itemize}

The implementation of the tools and the subsequent comparison is done using the following steps in the following order, and can be summarised as in table \ref{table:approach}:
\begin{itemize}
	\item Implemented base premise selection tool.
	\item Implemented various performance metrics, as described by Kaliszyk \cite{kaliszyk2014machine}.
	\item Implemented \crossvalidation in order to make proper performance analysis.
    \item Select and perhaps combine various \machinelearning methods, such as \nb or \ensemble.
	\item Select and extract various other features, such as number of conjunctions.
    \item Analyse which combination of features and \machinelearning methods work best for which corpus, and how well these configurations generalize.
\end{itemize}

The selection of features and \machinelearning techniques will occur during the research itself.
The corpora used are determined beforehand.

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[auto, node distance=2.5cm, main/.style={draw,align=center}]
		\node[main] (coq) {Coq term\\inside coq};
		\node[main] (xml) [below of=coq] {XML\\representation};
		\node[main] (term) [below of=xml] {Coq term $S$};
		\node[main] (summary) [below of=term] {Summary\\$<p, p_t, p_A>$};
		\node[main] (mapping) [left=1cm of summary] {Mapping\\$p \rightarrow p'$};
		\node[main] (definitions) [below right=2cm and 0cm of summary] {Definitions $D$};
		\node[main] (theorems) [below left=2cm and 0cm of summary] {Theorems $T$};
		\node[main] (features) [below of=definitions] {Features\\of theorems};
		\node[main] (dependencies) [below of=theorems] {Dependencies\\of theorems};
		\node[main] (prediction) [below left of=features] {Prediction};

		\draw[->] (coq) edge node {Coq XML export} (xml);
		\draw[->] (xml) edge node {Parser} (term);
		\draw[->] (term) edge node {Resolver} (summary);
		\draw[->] (term) edge node {} (mapping);
		\draw[->] (mapping) edge node {} (summary);
		\draw[->] (summary) edge node [right] {$p_A$} (definitions);
		\draw[->] (summary) edge node [left] {$p_t / p_A$} (theorems);
		\draw[->] (definitions) edge node {} (theorems);
		\draw[->] (theorems) edge node {} (features);
		\draw[->] (theorems) edge node {} (dependencies);
		\draw[->] (features) edge node {(External) learner} (prediction);
		\draw[->] (dependencies) edge node {} (prediction);
		
		\draw[dashed] (definitions) edge node [sloped, above] {$=$} (features);
		\draw[dashed] (summary) edge [bend left, pos=0.19] node {$p_t$} (dependencies);
	\end{tikzpicture}
	\label{table:transformations}
	\caption{Transformations performed by the \premiseselection tooling.}
\end{figure}


\subsection{Tooling}
Various design goals of the \premiseselection tool are to:
\begin{itemize}
    \item Support offline learning and analysis of \machinelearning on the various corpora.
    \item Enable integration in the \coqide GUI.
    	I would like to create something like Clippy or Vigor for \coq.
    \item Enable merging of the premise selection tool in the \coq main branch as a plugin.
\end{itemize}
In order to achieve this, I decided to implement the tool on top of the \acic datastructures
as exported by the \xml plugin in \coq.
As far as reasonable the \premiseselection tooling will be written in \ocaml.
I foresee that some effort will be required to interface \ocaml with existing implementations
of canonical \machinelearning algorithms.
The \machinelearning field favours \python and \matlab as the preferred development environments.
Ideally the tooling created can be used as a building block in a future \coq tactic which finishes proofs fully automatically.

\subsection{Extraction}
The \coq XML plugin exports various objects.
After parsing the XML output only some of these constructs are used.
Of these, the following \coq objects are used:
\begin{description}
    \item[(Co)Inductive definitions]
        \coq allows for (co)inductive definitions.
        These definitions are composed of a name, a type, and a list of constructors.
        Each constructor also is composed of a name and a type.
        A constructor is a valid definition which can be used in theorems.
		A (Co)Inductive definition also yields an induction principle (\texttt{ind}) and a recursion scheme (\texttt{rec}).
		These are also separately exported by \coq, and thus need not be considered further.

    \item[Constants (definitions / theorems / axioms)]
        The types and bodies of constants are defined separately, as needed.
        Theorems are proof-irrelevant, whilst definitions need to be substituted when applied.
        These objects they become undistinguishable when exported.
        Axioms only generate a type, and no body.
\end{description}

If time permits, also the following objects could be used:
\begin{description}
    \item[Proof in progress]
        Consists of a name, a type, a body and a list of dependencies.
        These dependencies still need to be satisfied to complete the proof.
    \item[Tactics application]
        On a higher level, tactics can be applied in order to form a proof.
        These higher level constructs are dependant on the proof engine.
        A proof consisting of tactics can thus become invalid given another \coq version.
        For premise selection these tactics can help solve proofs more quickly, because the proofs they form are smaller.
\end{description}

\coq also exports variables, but these are not used.
A variable consists of a name and a type, and becomes a parameter when a theory which uses the variable is applied.
Thusfar there is no use for these variables within this project.

\subsection{Features and output}
\label{section:features}
When performing \premiseselection, only theorems are regarded.
For other definitions selecting premises is conceptually problematic.
As the training set, for each proven theorem the features are computed from the type.
The definitions used in the body of a theorem are the desired result of the \premiseselection.
Hence, given the type of a new theorem to be proved, the \premiseselection yields a set of definitions which are likely to help prove the theorem.
These definitions are ordered by the chance the definition can be used in the proof.
This ordering is derived from the used definitions of similar complete theorems.
The similarity of complete theorems is captured in the model employed in the \premiseselection, and is derived from the features.

Distinguishing theorems from other definitions is not trivial.
Normally only definitions of kind \prop need to be considered as theorems.
However in the case of \corn the propositions are of type \cprop, which is of kind \kindtype.
Also the kind of a definition is not exported consistently by \coq.
Instead I will initially use a heuristic as defined by Kaliszyk \cite{kaliszyk2014machine}.
This heuristic defines the theorems, or the set of allowed dependencies, as all definitions except those used in the types of definitions.

For the prototype only the definitions used in the theorem type are regarded as features.
Concretely, for a set of proven theorems with their types and bodies, the used definitions from the types are collected.
For this set of proven theorems, a feature matrix $F$ is constructed.
The \texttt{true} boolean value $F_{ij}$ then represents the usage of definition $j$ in the type of the proven theorem $i$.

For the body of each proven theorem the used definitions are similarly collected, and a dependency matrix $D$ is constructed.
This matrix thus encodes the usage of definitions in the body of those proven theorems.

The predictor can now, for a unproven theorem, compute the features from the type and yield the ordered set of definitions likely to be useful in the to be constructed body.
After the prototype is completed additional features will be considered and implemented.
Most notably some corpus specific features will be tried.

\subsection{Machine learning}

\subsubsection{\knn}
$$
\begin{array}{lcl}
	F_s & : & T \rightarrow \mathbb{R} \\
	F_s(t) & = & T_s^\#(t)
\end{array}
$$
$$ \text{dist}(x, y) = \left( \sum_t^T \left( F_x(t) - F_y(t) \right)^2 \right)^{\frac{1}{2}} $$

\subsubsection{\nb}

\subsection{Example}
\begin{lstlisting}[language=Coq, mathescape]
Inductive nat : Set :=
  | O : nat
  | S : nat -> nat.

Definition nat_id := $\ldots$ .

Fixpoint plus (n m : nat) : nat :=
  match n with
  | O => m
  | S p => S (p + m)
  end.

Lemma plus_0_r : $\forall$ x , plus x 0 = x.
Proof.
$\ldots$ nat_ind $\ldots$
Qed.
Definition plus_0_r n := eq_sym (nat_ind _ eq_refl $\ldots$ ).
\end{lstlisting}

\begin{lstlisting}[language=Coq, mathescape, frame=none]
$S = \{$
	nat : Set$,$
	O : nat$,$
	S : nat -> nat$,$
	nat_ind : $\forall$ P : nat -> Prop, P 0 -> ($\forall$ n : nat, P n -> P (S n)) -> $\forall$ n : nat, P n := fix $\ldots$$,$
	plus : nat -> nat -> nat := fix $\ldots$$,$
	plus_0_r : $\forall$ x , plus x 0 = x := $\ldots$ nat_ind $\ldots$ $,$
$\} $
\end{lstlisting}

\begin{lstlisting}[language=Coq, mathescape, frame=none]
$D = \{$nat$,$ 0$,$ S$,$ plus$\}$
\end{lstlisting}

\begin{lstlisting}[language=Coq, mathescape, frame=none]
$T = \mathtt{dom}(S) \setminus D = \{$nat_ind$,$ plus_0_r$\}$
\end{lstlisting}

\subsection{Corpora}
\begin{description}
    \item[\compcert]
    \item[\formalin]
    \item[\corn]
    \item[\mathcomp]
\end{description}

\subsection{Metrics}
The performance of a predictor is measured using 

\begin{definition}{100Cover}
\end{definition}

\begin{definition}{100Precision}
\end{definition}

\begin{definition}{Recall}
\end{definition}

\begin{definition}{Rank}
\end{definition}

\begin{definition}{Area Under Curve (AUC)}
\end{definition}

% Cross validation on Corpora
% `Proof in Progress` -> step (aconstr)
% Subset corpora (Train) -> Corpus (Test) -> Rating
% A corpus is divided into Proofs in Progress (each substep)
% Ratio of guessed steps 
