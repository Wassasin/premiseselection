The premise selection problem is similar to \ltr in information retrieval
as in both cases relevant objects are sought from a large corpus.
Ordering these objects by relevance (or `rank') is also a common theme in both fields of study.

In order to determine in what manner these problems differ, and whether solutions
from information retrieval could be transformed to also work for the premise selection problem,
the Adarank algorithm \cite{xu2007adarank} is used as a showcase.

\ltr is typically used in search engines to search for relevant documents.
Given a search query $\query \in \queries$ composed of several terms, a preselection on the set of
all documents is made. Given a document in this subset $\doc \in \docs{\query}$,
the documents are sorted by rank $\rankdoc{\doc} \in \rat$.
The rank is determined for a document relative to the query.
Typically a document which mainly discusses terms from the query is rated highly.
The information retrieval method determines how this rank is computed.

Premise selection has neither the concepts of queries nor documents.

\begin{definition}
  Queries are formed from the features of each object $s \in \trainset$.
  \[
    \forall_{s \in \trainset} \features{}{s} \in \queries
  \]
\end{definition}

\begin{definition}
  We define the documents (that which we seek to rank) to be analogous to our dependencies.
  \[
    \forall_{\query \in \queries}~ \docs{\query} \subseteq \depset
  \]
\end{definition}

For premise selection, we search for useful dependencies given a conjecture $c \in \types$.
This means that we reason over the types of objects $\objs$,
instead of over the types of the dependencies as is the case with information retrieval.
Thus in order to transform to \ltr, we need to reason over the types of objects related to each dependency.

\begin{definition}
  For dependency $d \in \depsym$ we define the features $G_d$ to be the sum of
  the features $i \in \featurekeys$ of the parents $\parents[d]$ of $d$:
  \[
    G_d(i) = \sum_{p \in \parents[d]} \features{}{p}(i)
  \]
\end{definition}

\begin{definition}
  We tune the performance of a ranking using any function of type
  \[
    E :
      \rankings \rightarrow \objs \rightarrow [ -1, 1 ]
  \]

  For our experiments we use \oocover as a basis for our performance measurements,
  which yields results in the domain $[ 0, 1 ]$.

  \[
    E(r, s) = \oocoverf{r}{s}
  \]
\end{definition}

\begin{definition}
  In the original Adarank paper \cite{xu2007adarank} a permutation function is defined, which for
  our translation becomes trivial:

  \[ \pi : \queries \rightarrow 2^\depsym \rightarrow (\depsym \rightarrow \rat) \rightarrow \rankings \]
  \[ \pi(q, \docs{q}, f) = \lambda d \in \docs{q} . f(d) \]
\end{definition}

\begin{definition}
  Also for $t \in \nat$ a weak ranker $h_t$ is defined, which chooses the feature $x \in \irfeatures$
  that has the optimal weighted performance given our metric function $E$:

  \[
    h_t = \text{max}_k \sum_{s \in \trainset}
      P_t(\type[s]) \cdot E(r_k(\type[s]), s)
  \]
  \[
    \text{with}~ r_k(q) = \pi(q, \docs{q}, \psi_k(q, \docs{q}))
  \]
\end{definition}

\begin{definition}
  Given a query $\query \in \queries$ and document $d \in \docs{\query}$ the
  associated feature vector ${\vec{x}}_{q d} \in \irfeatures$ is defined as:

  \[
  \begin{array}{l}
    \psi_0(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(c(w_i, d) + 1) \\
    \psi_1(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(\frac{|C|}{c(w_i, C)} + 1) \\
    \psi_2(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(\text{idf}(w_i)) \\
    \psi_3(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(\frac{c(w_i, d)}{|d|} + 1) \\
    \psi_4(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(\frac{c(w_i, d)}{|d|} \cdot \text{idf}(w_i) + 1) \\
    \psi_5(q, d) = \sum_{w_i \in q \bigcap d} \text{ln}(\frac{c(w_i, d) \cdot |C|}{|d| \cdot c(w_i, C)} + 1) \\
    \psi_6(q, d) = \text{ln}( \text{BM25 score} )
  \end{array}
  \]

  \[
    {\vec{x}}_{q d} = \Psi(q, d) = \left\{ \psi_0(q, d), ~\ldots~, \psi_6(q, d) \right\}
  \]
\end{definition}

\begin{definition}
  \[ c(w, d) \]
\end{definition}

\begin{definition}
  \[ C \]
\end{definition}

\begin{definition}
  \[ \text{idf}(\cdot) \]
\end{definition}

\begin{definition}
  \[ \text{BM25 score} \]
\end{definition}

\[
  \alpha_t = \frac{1}{2} \cdot \ln \frac{
    \sum^m_{i=1} P_t(i) \{ 1 + E(\pi(q_i, \docs{i}, h_t), y_i) \}
  }{
    t
  }
\]
