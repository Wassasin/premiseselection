The premise selection problem is similar to \ltr in information retrieval
as in both cases relevant objects are sought from a large corpus.
Ordering these objects by relevance (or `rank') is also a common theme in both fields of study.

In order to determine in what manner these problems differ, and whether solutions
from information retrieval could be transformed to also work for the premise selection problem,
the Adarank algorithm \cite{xu2007adarank} is used as a showcase.

\ltr is typically used in search engines to search for relevant documents.
Given a search query $\query \in \queries$ composed of several terms, a preselection on the set of
all documents is made. Given a document in this subset $\doc \in \docs{\query}$,
the documents are sorted by rank.
The rank is determined for a document relative to the query.
Typically a document which mainly discusses terms from the query is rated highly.
The information retrieval method determines how this rank is computed.

Premise selection has neither the concepts of queries nor documents.
However a transformation can be applied, as the concepts are somewhat similar.

\begin{definition}
  Queries are formed from the features of each object $s \in \objsym$.
  \[
    \forall_{s \in \objsym} \features{}{s} \in \queries
  \]
\end{definition}

\begin{definition}
  We define the documents (that which we seek to rank) to be analogous to our dependencies.
  \[
    \forall_{\query \in \queries}~ \docs{\query} \subseteq \depset
  \]
\end{definition}

For premise selection, we search for useful dependencies given a conjecture $c \in \types$.
This means that we reason over the types of objects $\objs$,
instead of over the types of the dependencies as is the case with information retrieval.
Thus in order to transform to \ltr, we need to reason over the types of objects related to each dependency.

\begin{definition}
  For dependency $d \in \depsym$ we define the features $G_d$ per single feature
  $i \in \featurekeys$ the be the sum of the number of occurrences of that feature
  in the parents $\parents[d]$ of $d$:
  \[
    G_d(i) = \sum_{p \in \parents[d]} \features{}{p}(i)
  \]
\end{definition}

\begin{definition}
  We tune the performance of a ranking using any function of type
  \[
    E :
      \rankings \rightarrow \objs \rightarrow [ -1, 1 ]
  \]

  For our experiments we use \oocover as a basis for our performance measurements,
  which yields results in the domain $[ 0, 1 ]$.

  \[
    E(r, s) = \oocoverf{r}{s}
  \]
\end{definition}

\begin{definition}
  Given a query $\query \in \queries$ and document $d \in \docs{\query}$ the
  associated feature vector ${\vec{x}}_{q d} \in \irfeatures$ is defined as:

  \[
  \begin{array}{l}
    \psi_0(q, d) = \sum_{w \in q \bigcap d} \text{ln}(c(w, d) + 1) \\
    \psi_1(q, d) = \sum_{w \in q \bigcap d} \text{ln}(\frac{|C|}{c(w, C)} + 1) \\
    \psi_2(q, d) = \sum_{w \in q \bigcap d} \text{ln}(\text{idf}(w)) \\
    \psi_3(q, d) = \sum_{w \in q \bigcap d} \text{ln}(\frac{c(w, d)}{|d|} + 1) \\
    \psi_4(q, d) = \sum_{w \in q \bigcap d} \text{ln}(\frac{c(w, d)}{|d|} \cdot \text{idf}(w) + 1) \\
    \psi_5(q, d) = \sum_{w \in q \bigcap d} \text{ln}(\frac{c(w, d) \cdot |C|}{|d| \cdot c(w, C)} + 1) \\
    \psi_6(q, d) = \ln( \text{BM25-score}(q, d) )
  \end{array}
  \]

  \[
    {\vec{x}}_{q d} = \Psi(q, d) = \left[ \psi_0(q, d), ~\ldots~, \psi_6(q, d) \right]
  \]
\end{definition}

\begin{definition}
  We denote the frequency of word $w \in \featurekeys$ in document $d \in \depsym$ as follows:
  \[ c(w, d) = \frac{G_d(w)}{ \sum_{v \in \featurekeys} G_d(v) }\]
\end{definition}

\begin{definition}
  We use $C$ to denote the collection of words in the entire collection of documents.
  Thus if we use the following the denote the frequency of word $w \in \featurekeys$
  in the entire collection of documents:
  \[ c(w, C) =
    \left( \sum_{d \in \depsym} G_d(w) \right) /
    \left( \sum_{d \in \depsym} \sum_{v \in \featurekeys} G_d(v) \right)
  \]
\end{definition}

\begin{definition} The \emph{inverse document frequency} is a measure that indicates
  common a word is across all documents.
  The frequency is determined by counting how many documents feature the word
  at least once.
  \[ \text{idf}(w) = \log(\frac{|\depsym|}{ 1 + \left| \{ d \in \depsym ~|~ G(d, w) > 0 \} \right|}) \]
\end{definition}

\begin{definition}
  The \emph{Okapi BM25-score} \cite{robertson1995okapi} is used as a final feature:
  \[
    \text{BM25-score}(q, d) = \sum_{w \in q} \text{idf}(w) \cdot
    \frac{c(w, d) \cdot (k_1 + 1)}
      {c(w, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}} )}
  \]

  \[ \text{with}~ k_1 = 1.5 ~\text{and}~ b = 0.75 \]
  \[ \text{and}~ \text{avgdl} = \frac{ \sum_{d \in \depsym} |d| }{|\depsym|} \]
\end{definition}

\begin{definition}
  In the original Adarank paper \cite{xu2007adarank} a permutation function is defined,
  which computes the ranking given the features for document $d \in \docs{q}$:

  \[ \pi : \queries \rightarrow 2^\depsym \rightarrow (\irfeatures \rightarrow \rat) \rightarrow \rankings \]
  \[ \pi(q, \docs{q}, f) = \lambda d \in \docs{q} . f(\Psi(q, d)) \]
\end{definition}

Adarank is parameterized in the number of rounds $T \in \nat$ it should run for.
Each round a weak ranker $h_t$, it's weight $\alpha_t$, a strong ranker $f_t$
and a weight distribution $P_t : \objsym \rightarrow \rat$ is computed.

\begin{definition}
  Initially the weight distribution $P_0$ is defined as follows:
  $$ P_0(s) = \frac{1}{|\objsym|} $$
\end{definition}

\begin{definition}
  For round $t \in \nat$ a weak ranker $h_t : \irfeatures \rightarrow \rat$ is defined, which chooses the feature
  that has the optimal weighted performance given our metric function $E$:

  $$ h_t(\vec{x}) = \vec{x}[i] $$
  with $$ i = \max_k \sum_{s \in \objsym} P_t(\type[s]) \cdot E(r_k(\type[s]), s) $$
  $$ r_k(q) = \pi(q, \docs{q}, \psi_k(q)) $$
\end{definition}

\begin{definition}
  For round $t \in \nat$ the weight $\alpha_t$ of a weak ranker $h_t$ is determined as follows:
  \[
    \alpha_t = \frac{1}{2} \cdot \ln \frac{
      \sum_{s \in \objsym} P_t(\type[s]) \cdot ( 1 + E(\pi(\type[s], \docs{\type[s]}, h_t), s) )
    }{
      \sum_{s \in \objsym} P_t(\type[s]) \cdot ( 1 - E(\pi(\type[s], \docs{\type[s]}, h_t), s) )
    }
  \]
\end{definition}

\begin{definition}
  Similarly for round $t \in \nat$ the strong ranker is defined to be the weighted
  sums of all weak rankers of all previous rounds:
  $$ f_t(\vec{x}) = \sum_{k=0}^{t} \alpha_k h_k(\vec{x}) $$
\end{definition}

\begin{definition}
  The weight distribution for each subsequent round with $s \in \objsym$ is defined as follows:
  $$
    P_{t+1}(s) = \frac{
      \exp(- E(\pi(\type[s], \docs{\type[s]}, f_t), s))
    }{
      \sum_{s' \in \objsym} \exp(- E(\pi(\type[s'], \docs{\type[s']}, f_t), s'))
    }
  $$
\end{definition}

\begin{definition}
  We define the final ranker $f(\vec{x}) : \irfeatures \rightarrow \rat$ to be:
  $$ f(\vec{x}) = f_T(\vec{x})$$
\end{definition}

\begin{definition}
  In order to yield an actual predictor we need to convert a conjecture $c \in \types$
  to a query and compute the rank for the computed features of the dependency candidate $\phi \in \depsym$:
  $$ P_{\text{adarank}}^T(c, \phi) = f_T(\Psi(c, \phi)) $$
\end{definition}
