\label{section:results}

We use the \corn dataset with the frequency model and the pessimistic strategy as a baseline for all comparisons.
For a select logical variables various values are examined if applicable.
First we see whether the use of prior knowledge affects the resulting performance.
Secondly we verify whether our chosen poset consistency strategy works as intended.
Thirdly we investigate whether the use of the flat, frequency or depth model works best.
Fourth we iterate over all machine learning methods and how they most optimally can be configured.
Finally we inspect all corpora and see how various configurations of machine learners perform.

\subsection{Prior knowledge}

\todo{figure depicting non-cv prior dataset}
\todo{explanation in method?}

\begin{figure}[H]
  \centering
  \begin{tabular}{r|cll}
    Method & mode & \oocover & \auc \\\hline
    \input{data/prior-vs-nonprior-corn-frequency-pessimistic}
  \end{tabular}
  \caption{Results for the \corn (frequency) dataset with pessimistic strategy, with prior knowledge compared to without}
\end{figure}

This result is rather unexpected.
The models that have not been trained on prior corpora (being \coq and \mathclasses) outperform
those that have been.
Normally we would expect more context to provide more suitable material to make an educated guess.
Instead more noise was introduced into the models.
It shows that the machine learning methods have a problem capturing the proper context.
For \knn without prior knowledge performs better for a lower number of neighbors $k$.
For \nb without prior knowledge performs better with a lower rewarding constant $s$ for known features.
Finally \adarank performs better with a lower number of rounds $T$.
This is at least indicative that the trainingset is smaller, which indeed it is.

\begin{figure}[H]
  \centering
  \includegraphics{prior-knn-corn-frequency-pessimistic-oocover.pdf}
  \caption{\oocover of the \knn method for both the prior and non-prior versions of the \corn dataset for different values of $k$}
\end{figure}

When looking into the performance of the \knn method we see that it has the same behaviour regardless which dataset you have, except for
the slightly lower performance for the prior dataset.

\subsection{Poset consistency}
\begin{figure}[H]
  \centering
  \includegraphics{posetcons-knn-corn-frequency-oocover.pdf}
  \caption{\oocover of the \knn method for the \corn dataset for different values of $k$ and various strategies}
\end{figure}

As expected the optimistic strategy does not perform well at all.
We conjecture that, contrary to the previous results, a lack of context results in this bad performance.
It might be argueable that the pessimistic strategy does not remove enough information to fairly compare to the previous study by
Kaliszyk. \cite{kaliszyk2014machine}

\subsection{Frequency, depth and flat}
\begin{figure}[H]
  \centering
  \includegraphics{field-knn-corn-pessimistic-oocover.pdf}
  \caption{\oocover of the \knn method with pessimistic strategy for the \corn dataset for different values of $k$}
\end{figure}

Interestingly the depth model outperforms the frequency model by quite some margin.
It might be the case that this model is too descriptive and invites overfitting.
If not so, than might prove to be even better with the frequency model combined.

\subsection{Methods}

\subsubsection{\knn}
\begin{figure}[H]
  \centering
  \includegraphics{dataset-knn-frequency-pessimistic-oocover.pdf}
  \caption{\oocover of the \knn method with pessimistic strategy for various datasets for different values of $k$}
\end{figure}

Peak \oocover performance for each corpus occurs between $K=20$ and $K=50$.
\corn and \mathclasses perform notably worse than the rest.
After a sharp ascent, a slow decrease in performance occurs after peak performance.
For low values of $K$ not enough suggestions are made.
After the peak too many suggestions are made, and the relevant suggestions are more frequently made after the first one hundred.

\subsubsection{\nb}
We have run \nb for a vast range of parameters.
Graphs are rendered with interpolation enabled, computed with steps of $\Delta_\pi = 8$ and $\Delta_\tau = \Delta_\sigma = 2$.
Most notably for the $\pi$-$\tau$-graph the edges are less fuzzy than depicted.

\begin{figure}[H]
  \centering
  \includegraphics{dataset-nb-frequency-pessimistic-oocover.pdf}
  \caption{\oocover of the \nb method for the \corn (frequency) dataset for $\sigma = -15$ or $\tau = 0$}
\end{figure}

As expected, any value for $\tau \neq 0$ does not work properly.
This implies that the implementation differs with the work by Kuhlwein \cite{kuhlwein2013mash} and Kaliszyk \cite{kaliszyk2014machine},
as they recommend to use $\pi = 10$, $\sigma = -15$ and $\tau = 20$.
Any value for $\pi \geq 10$ and $\sigma \leq -15$ seems to work fine.

\subsubsection{\adarank}
The \adarank method is parameterized in the number of rounds $T$ the learning algorithm is run for.
This parameter is almost irrelevant for the computational time required to compute the performance on a testset.
Also, in the learning algorithm most of the time is spent in computing values derived from the pairs of documents, and not in computing the rounds themselves.
We plot the \oocover performance of \adarank parameterized in $T$ for several datasets:

\begin{figure}[H]
  \centering
  \includegraphics{dataset-adarank-frequency-pessimistic-oocover.pdf}
  \caption{\oocover of the \adarank method for various (frequency) datasets for various values of $T$}
\end{figure}

A dip can be seen for $T=4$ and another relatively big improvement for $T=6$, especially for \corn and \mathcomp.
Given that a high value of $T$ has almost no impact on computational resources required, the highest value for $T$ seems to be the best choice.
As for each trainingset the derived features are ordered separately, it is not possible to state which derived feature performs best using this approach.
This can be done as future work.

\begin{figure}[H]
  \centering
  \begin{tabular}{r|rrrrrr}
    T & \oocover & \ooprecision & \recall & \rank & \auc & \volume \\\hline
    \input{data/adarank-corn-frequency-pessimistic}
  \end{tabular}
  \caption{Performance of the \adarank method for the \corn (frequency) dataset}
\end{figure}

Interestingly for $T=5$ the volume of suggestions is reduced by $12\%$, but the \oocover is not affected.
Thus it succesfully discourages invalid suggestions.
The \auc is marginally impacted negatively, so the order changes adversely slightly.

\subsection{Datasets}

Per dataset we've ran a batch of various machine learning methods and various parameter configurations.
For each dataset we list the best performing parameter configuration.
We can easily read which method performs best, but hopefully will also be able to infer aspects of the nature of the datasets.

For this we've added the \emph{Omniscient} method, which is a cheating method that knows all aspects of the testset beforehand.
It will try to formulate the optimal answer to the premise selection problem.
By measuring the performance of these answers, we know the upper bound performance that a machine learning method can reach for that given dataset.

\subsubsection{\coq}

\begin{figure}[H]
  \centering
  \begin{tabular}{r|rrrrrr}
    Method & \oocover & \ooprecision & \recall & \rank & \auc & \volume \\\hline
    \input{data/best-coq-frequency-pessimistic}
  \end{tabular}
  \caption{Performance of the various methods for the \coq (frequency) dataset}
\end{figure}

Adarank clearly performs best, with an outstanding performance on \auc.
Though \auc is better if the \volume is higher, which is not necessarily useful.

\subsubsection{\corn}
\begin{figure}[H]
  \centering
  \begin{tabular}{r|rrrrrr}
    Method & \oocover & \ooprecision & \recall & \rank & \auc & \volume \\\hline
    \input{data/best-corn-frequency-pessimistic}
  \end{tabular}
  \caption{Performance of the various methods for the \corn (frequency) dataset}
\end{figure}

Relative to the \coq corpus we perform a lot worse with the \corn dataset.
Note that \adarank still performs reasonably well for \oocover, but also \nb and \ensemble perform quite well on \auc.

\subsubsection{\mathclasses}
\begin{figure}[H]
  \centering
  \begin{tabular}{r|rrrrrr}
    Method & \oocover & \ooprecision & \recall & \rank & \auc & \volume \\\hline
    \input{data/best-mathclasses-frequency-pessimistic}
  \end{tabular}
  \caption{Performance of the various methods for the \mathclasses (frequency) dataset}
\end{figure}

The \oocover performance for the \mathclasses corpus is quite similar to that of the \coq corpus.
This is very interesting, because that means that all complexity of the \corn dataset is included in that dataset, and not in its dependency \mathclasses.
The \auc performance is comparable to that for \corn.

\subsubsection{\mathcomp}
\begin{figure}[H]
  \centering
  \begin{tabular}{r|rrrrrr}
    Method & \oocover & \ooprecision & \recall & \rank & \auc & \volume \\\hline
    \input{data/best-mathcomp-frequency-pessimistic}
  \end{tabular}
  \caption{Performance of the various methods for the \mathcomp (frequency) dataset}
\end{figure}

We perform worst on the \mathcomp corpus on the \oocover metric.
The \auc metric is comparable to our \coq performance.
Notably the \omniscient method does not get a perfect \oocover grading.
This means that there are a few theorems that use more than a hundred definitions.