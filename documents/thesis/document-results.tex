\label{section:results}

We use the \corn dataset with the frequency model and the canonical strategy as a baseline for all comparisons.
For a select logical variables various values are examined if applicable.
First we see whether the use of prior knowledge affects the resulting performance.
Secondly we verify whether our chosen poset consistency strategy works as intended.
Thirdly we investigate whether the use of the flat, frequency or depth model works best.
Fourth we iterate over all machine learning methods and how they most optimally can be configured.
Finally we inspect all corpora and see how various configurations of machine learners perform.

\subsection{Prior knowledge}
\label{section:prior}
First we examine in which manner prior knowledge affects the performance of the various machine learning methods.
Prior knowledge in this case means that we also train the models with the prerequisite datasets.
We only examine the \corn dataset in this section.
Thus for these results we've also trained the \emph{prior} models with the \coq standard library and \mathclasses.
We expect that having this prior knowledge provides more context to the datasets, and will thus result in a better performance.

\begin{figure}[H]
  \centering
  \begin{tabular}{r|cll}
    Method & mode & \oocover & \auc \\\hline
    \input{data/prior-vs-nonprior-corn-frequency-canonical}
  \end{tabular}
  \caption{Results for the \corn (frequency) dataset with canonical strategy, with prior knowledge compared to without}
\end{figure}

This result however is not consistent with regards to our expectations.
The models that have not been trained on prior corpora (being \coq and \mathclasses) outperform
those that have been.
The only exception to this is the \adarank method, but only by a small margin.
Normally we would expect more context to provide more suitable material to make an educated guess.
Instead we think more noise was introduced into the models.
It shows that the machine learning methods have a problem capturing the proper context in this case.
For \knn the dataset without prior knowledge performs better for a lower number of neighbors $k$.
For \nb the dataset without prior knowledge performs better with a lower rewarding constant $s$ for known features.
Finally \adarank performs better with a lower number of rounds $T$.
These facts are at least indicative of a smaller trainingset, which indeed the trainingset is.

\begin{figure}[H]
  \centerline{\includegraphics{prior-knn-corn-frequency-canonical-oocover.pdf}}
  \caption{\oocover of the \knn method for both the prior and non-prior versions of the \corn dataset for different values of $k$}
\end{figure}

When looking into the performance of the \knn method we see that it has the same behaviour regardless which dataset you have, except for
the slightly lower performance for the prior dataset.

\subsection{Poset consistency}
\label{section:poset-consistency}
Previously we have formulated various strategies to filter out illegal knowledge from our trainingset.
While these strategies may be better than the other in a performance sense, they should not be viewed in that light.
Rather, some strategies should be regarded as being better suited in an academic sense.
Whilst the canonical method is more consistent with regards to the research by Kaliszyk \cite{kaliszyk2014machine},
the pessimistic method eliminates the factor of the order of the trainingset, which makes comparing results more fair.
We now compare the performance between these strategies in order to validate our understanding of their relative effect on performance.

\begin{figure}[H]
  \centerline{\includegraphics{posetcons-knn-corn-frequency-oocover.pdf}}
  \caption{\oocover of the \knn method for the \corn dataset for different values of $k$ and various strategies}
\end{figure}

As expected the optimistic strategy does not perform well at all.
We conjecture that a lack of context results in this bad performance, which is somewhat contrary to the previous results.
As expected the pessimistic strategy performs better than the canonical method, as it removes the bare minimum of illegal knowledge
from the test set.

\subsection{Frequency, depth and flat}
The original research by Kaliszyk only uses a frequency-based model of the dataset.
It is hard to say whether a depth-based model or a flat model will be better.
We expect that a depth-based model is more indicative of the type of solutions that exist within a definition,
and will thus perform better.

\begin{figure}[H]
  \centerline{\includegraphics{field-knn-corn-canonical-oocover.pdf}}
  \caption{\oocover of the \knn method for the \corn dataset for different values of $k$}
\end{figure}

Interestingly the depth model outperforms the frequency model by quite some margin.
It might be the case that this model is too descriptive and invites overfitting.
If not so, than it might prove to be even better with the frequency model combined in some kind of ensemble.

\subsection{Methods}
% we iterate over all machine learning methods and how they most optimally can be configured.
For this thesis we implemented various machine learning methods.
These methods have varying parameters that can be configured.
In this section we will explore how various configurations of these methods affect performance.

\subsubsection{\knn}
\begin{figure}[H]
  \centerline{\includegraphics{dataset-knn-frequency-canonical-oocover.pdf}}
  \caption{\oocover of the \knn method for various datasets for different values of $k$}
\end{figure}

For the \knn method peak \oocover performance for each corpus occurs after $K\geq20$.
\corn and \mathclasses perform notably worse than the rest.
After a sharp ascent, a slow stagnation or decrease in performance occurs after peak performance.
For low values of $K$ not enough suggestions are made.
After the peak too many suggestions are made, and the relevant suggestions are more frequently made after the first one hundred.

\subsubsection{\nb}
The \nb method has three parameters: a baseline value $\tau$, a rewarding constant $\pi$ and punishing constant $\sigma$.
We have run \nb for a vast range of these parameters in order to get a feeling on how they affect performance.
The results of this have been plotted in graphs, which are rendered with interpolation enabled,
computed with steps of $\Delta_\pi = 8$ and $\Delta_\tau = \Delta_\sigma = 2$.

\begin{figure}[H]
  \centerline{\includegraphics{dataset-nb-frequency-canonical-oocover.pdf}}
  \caption{\oocover of the \nb method for the \corn (frequency) dataset for $\sigma = -15$ or $\tau = 0$}
\end{figure}

As expected, any value for $\tau \neq 0$ does not work properly.
The interpolation depicts fuzzy edges for the $\pi$-$\tau$-graph, whilst actually for any value of $\tau \neq 0$ the performance
is radically worse.
This implies that the implementation differs with the work by Kuhlwein \cite{kuhlwein2013mash} and Kaliszyk \cite{kaliszyk2014machine},
as they recommend to use $\pi = 10$, $\sigma = -15$ and $\tau = 20$.
This is either a bug (in either implementation), or an underspecification in the other papers.
Any value for $\pi \geq 10$ and $\sigma \leq -15$ seems to work fine.

\subsubsection{\adarank}
The \adarank method is parameterized in the number of rounds $T$ the learning algorithm is run for.
This parameter is almost irrelevant for the computational time required to compute the performance on a testset.
Also, in the learning algorithm most of the time is spent in computing values derived from the pairs of documents, and not in computing the rounds themselves.
We plot the \oocover performance of \adarank parameterized in $T$ for several datasets:

\begin{figure}[H]
  \centerline{\includegraphics{dataset-adarank-frequency-canonical-oocover.pdf}}
  \caption{\oocover of the \adarank method for various (frequency) datasets for various values of $T$}
\end{figure}

A dip can be seen for $T=4$ and another relatively big improvement for $T=6$, especially for \corn and \mathcomp.
Given that a high value of $T$ has almost no impact on computational resources required, the highest value for $T$ seems to be the best choice.
As for each trainingset the derived features are ordered separately, it is not possible to state which derived feature performs best using this approach.
This can be done as future work.

\begin{figure}[H]
  \centering
  \begin{tabular}{r|rrrrrr}
    T & \oocover & \ooprecision & \recall & \rank & \auc & \volume \\\hline
    \input{data/adarank-corn-frequency-canonical}
  \end{tabular}
  \caption{Performance of the \adarank method for the \corn (frequency) dataset}
\end{figure}

Interestingly for $T=5$ the volume of suggestions is reduced by $6\%$, but the \oocover is not affected.
Thus it succesfully discourages invalid suggestions.
The \auc is marginally impacted negatively, so the order in which suggestions are made changes slightly for the worse.

\subsection{Datasets}

Per dataset we've ran a batch of various machine learning methods and various parameter configurations.
For each dataset we list the best performing parameter configuration.
We can easily read which method performs best, but hopefully will also be able to infer aspects of the nature of the datasets.
As a preview we have compiled the \oocover performance of each method and each corpus in Figure \ref{fig:best-oocover}.

\begin{figure}[H]
  \centerline{\includegraphics{datasets-best-oocover.pdf}}
  \caption{\oocover performance of the various methods for all (frequency) datasets}
  \label{fig:best-oocover}
\end{figure}

We have also added the \emph{Omniscient} method, which is a method that cheats by knowing all aspects of the testset beforehand.
It will try to formulate the optimal answer to the premise selection problem.
By measuring the performance of these answers, we know the upper bound performance that a machine learning method can reach for that given dataset.

\subsubsection{\coq}

\begin{figure}[H]
  \centering
  \begin{tabular}{r|rrrrrr}
    Method & \oocover & \ooprecision & \recall & \rank & \auc & \volume \\\hline
    \input{data/best-coq-frequency-canonical}
  \end{tabular}
  \caption{Performance of the various methods for the \coq (frequency) dataset}
\end{figure}

Adarank clearly performs best, with an outstanding performance on \auc.
The performance of \nb and \ensemble is respectable,
especially considering it has a similar \auc compared to \adarank,
but a far better \volume.

\subsubsection{\formalin}

\begin{figure}[H]
  \centering
  \begin{tabular}{r|rrrrrr}
    Method & \oocover & \ooprecision & \recall & \rank & \auc & \volume \\\hline
    \input{data/best-ch2o-frequency-canonical}
  \end{tabular}
  \caption{Performance of the various methods for the \formalin (frequency) dataset}
\end{figure}

The performance for \formalin is slighty better than the performance for \coq.
The volume is a bit lower across all methods.

\subsubsection{\corn}
\begin{figure}[H]
  \centering
  \begin{tabular}{r|rrrrrr}
    Method & \oocover & \ooprecision & \recall & \rank & \auc & \volume \\\hline
    \input{data/best-corn-frequency-canonical}
  \end{tabular}
  \caption{Performance of the various methods for the \corn (frequency) dataset}
\end{figure}

Relative to the \coq corpus we perform a lot worse with the \corn dataset.
Note that \adarank still performs reasonably well for \oocover, but also \nb and \ensemble perform quite well on \auc.
Whilst with previous corpii the \knnadaptive performance could keep up with the \knn performance, this is no longer the
case for the \corn dataset.

\subsubsection{\mathclasses}
\begin{figure}[H]
  \centering
  \begin{tabular}{r|rrrrrr}
    Method & \oocover & \ooprecision & \recall & \rank & \auc & \volume \\\hline
    \input{data/best-mathclasses-frequency-canonical}
  \end{tabular}
  \caption{Performance of the various methods for the \mathclasses (frequency) dataset}
\end{figure}

The \oocover performance for the \mathclasses corpus is quite similar to that of the \coq corpus.
This is very interesting, because that means that all complexity of the \corn dataset is included in that dataset, and not in its dependency \mathclasses.
The \auc performance is comparable to that for \corn.

\subsubsection{\mathcomp}
\begin{figure}[H]
  \centering
  \begin{tabular}{r|rrrrrr}
    Method & \oocover & \ooprecision & \recall & \rank & \auc & \volume \\\hline
    \input{data/best-mathcomp-frequency-canonical}
  \end{tabular}
  \caption{Performance of the various methods for the \mathcomp (frequency) dataset}
\end{figure}

We perform worst on the \mathcomp corpus on the \oocover metric.
The \auc metric is comparable to our \coq performance.
Notably the \omniscient method does not get a perfect \oocover grading.
This means that there are a few theorems that use more than a hundred definitions.